{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sys \n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "%matplotlib inline\n",
    "\n",
    "#filename = '/home/kp/Desktop/flow_a-l_0.3-time_1000.0hr.dat' # no bugs with this one at all \n",
    "filename = '/home/kp/Desktop/flow_a-l_0.6-time_1000.0hr.dat' # maybe this one has problems with it which trigger the bug\n",
    "entrainpath = filename[:-4]+'-entrains.dat'\n",
    "depositpath = filename[:-4]+'-deposits.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use pandas to load the file in for analysis. \n",
    "# pandas is nice because it's faster than np.loadtxt and it also supports chunking\n",
    "nrows = 5000000 # try 5 million rows \n",
    "#linecount = num_lines = sum(1 for line in open(filename))-2 # this takes time. \n",
    "#print('{} lines to process '.format(linecount))\n",
    "chunksize = int(50000) # take 20 million lines at once \n",
    "#print('{} chunks to iterate'.format(float(linecount)/chunksize))\n",
    "def loadfile(filename):\n",
    "    return pd.read_csv(filename, skiprows = 2, usecols = [0,1,2], header = None, \n",
    "                   delim_whitespace = True, index_col = False, iterator = True, chunksize = chunksize, nrows=nrows)\n",
    "file = loadfile(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def discern_ed(entrainpath, depositpath, filename):\n",
    "    # find all entrainment and deposition times within a file \n",
    "    # useful \n",
    "    # https://www.pythonforthelab.com/blog/introduction-to-storing-data-in-files/\n",
    "    file = loadfile(filename)\n",
    "    entrainfile = open(entrainpath, 'wb+') # w means write \n",
    "    depositfile = open(depositpath, 'wb+')\n",
    "    m0 = 50000 # the value of the top of the last chunk\n",
    "\n",
    "    i = 0 \n",
    "    for f in file: \n",
    "        i+=1\n",
    "\n",
    "        n,m,t = f.values.T # unpack the values from the chunk\n",
    "\n",
    "        # check for entrainment and deposition on the boundary element \n",
    "        if m[0]==m0+1: # if deposition occurs on lowest boundary of chunk\n",
    "            np.savetxt(depositfile, np.column_stack((t[0], m[0])))\n",
    "        elif m[0]==m0-1: # if entrainment occurs on lowest boundary of chunk\n",
    "            np.savetxt(entrainfile, np.column_stack((t[0], m[0])))\n",
    "        m0 = m[-1] # reset the boundary value for the next chunk \n",
    "\n",
    "        # now check for entrainment and deposition within the chunk. Need to exclude lower boundary\n",
    "        erodemask = m[1:]-np.roll(m,1)[1:]==-1  # indices into t[1:] where erosion occurred \n",
    "        depositmask = m[1:]-np.roll(m,1)[1:]==1 # indices into t[1:] where deposition occurred \n",
    "        # chunk save errything \n",
    "        data = np.array([t[1:][erodemask], m[1:][erodemask]]).T\n",
    "        np.savetxt(entrainfile, data)\n",
    "        entrainfile.flush()\n",
    "        data = np.array([t[1:][depositmask], m[1:][depositmask]]).T\n",
    "        np.savetxt(depositfile, data) \n",
    "        depositfile.flush()\n",
    "\n",
    "\n",
    "    entrainfile.close()\n",
    "    depositfile.close()\n",
    "    \n",
    "discern_ed(entrainpath,depositpath,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mstats(filename):\n",
    "    # data loader \n",
    "    load = lambda filename: pd.read_csv(filename, skiprows = 2, usecols = [0,1,2], header = None, \n",
    "                   delim_whitespace = True, index_col = False, iterator = True, chunksize = 50000,nrows=nrows)#, nrows=100)\n",
    "\n",
    "    file = load(filename)\n",
    "    i = 0 #keep track of which chunk you're on \n",
    "    chunked_mean = 0 # running sum of mean(m) values from each chunk \n",
    "    for chunk in file: \n",
    "        _, m, _ = chunk.values.T\n",
    "        chunked_mean += m.mean()\n",
    "        i+=1 # keep track of chunk number \n",
    "    mean_m = chunked_mean/i # the mean\n",
    "\n",
    "    i=0\n",
    "    chunked_var = 0 # running sum of (m-mean(m))**2/chunksize\n",
    "    file = load(filename)\n",
    "    for chunk in file:\n",
    "        _, m, _ = chunk.values.T\n",
    "        chunked_var += ((m-mean_m)**2).sum()/m.size # compute the variance of the chunk\n",
    "        i+=1\n",
    "    std_m = np.sqrt(chunked_var/i)\n",
    "\n",
    "    # compute all of the m values to consider.. go from mean(m)-3*std(m) to mean(m)+3*std(m) \n",
    "    mvals = np.arange(int(round(mean_m-3*std_m)), int(round(mean_m+3*std_m))+1, 1)\n",
    "    # so now you have the range of m values to consider\n",
    "\n",
    "    # compute the probabilities of each m value \n",
    "    file = load(filename) # load the iterators again\n",
    "    counts = []\n",
    "    for chunk in file:\n",
    "        _, m, _ = chunk.values.T\n",
    "        chunkcounts = []\n",
    "        for mv in mvals:\n",
    "            mvcounts = (m == mv).sum()\n",
    "            chunkcounts.append(mvcounts)\n",
    "        chunkcounts = np.array(chunkcounts)\n",
    "        counts.append(chunkcounts)\n",
    "    counts = np.array(counts)\n",
    "    counts = counts.sum(axis = 0)\n",
    "    pmvals = counts/counts.sum()\n",
    "    data = np.array((mvals, pmvals)).T\n",
    "    mstatsfile = filename[:-4]+'-mstats.dat'\n",
    "\n",
    "    np.savetxt(mstatsfile, data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m, pm = mstats(filename).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(m,pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax = 500*3600 # maximum return time to allow for \n",
    "dt = 0.5 # bin size \n",
    "bins = np.arange(0, tmax, dt) # bins \n",
    "t0 = time.time()\n",
    "def conditional_rt(entrainfile, depositfile, mstar, bins = bins, dt = 0.5, tmax = tmax):\n",
    "    \"\"\"compute the return time distribution conditional to elevation mstar\n",
    "    given entrainfile, the dat of all t, m at entrainment, and depositfile\n",
    "    the analogue for deposition\"\"\"\n",
    "    \n",
    "    # first compute all return times\n",
    "    returns = np.array([]) # compute all of the return times to mstar\n",
    "    for chunk in entrainfile: # iterate thru entrainment times and m values \n",
    "        te,me = chunk.values.T\n",
    "        chunk_times = te[me==mstar] # returns to mstar within the chunk\n",
    "        returns = np.concatenate((returns, chunk_times))\n",
    "    # then compute the departure times     \n",
    "    departures = np.array([]) # compute all of the departure times from mstar \n",
    "    for chunk in depositfile: # compute all of the departures from mstar \n",
    "        td,md = chunk.values.T\n",
    "        chunk_times = td[md==mstar+1]\n",
    "        departures = np.concatenate((departures, chunk_times))\n",
    "    \n",
    "    if ( len(returns) > 0 ) and ( len(departures) > 0 ):\n",
    "\n",
    "        while ( returns[0] < departures[0] ): # while first return smaller than first departure\n",
    "            returns = returns[1:] # crop off first return  \n",
    "        while ( departures[-1] > returns[-1] ): # while last departure greater than last return \n",
    "            departures = departures[:-1] # crop off last departure\n",
    "            \n",
    "        # at this point, the return time series starts from a departure and end from a return \n",
    "        # but there is no guarantee that it doesn't start with k departures or k returns \n",
    "        \n",
    "        # so if it isn't woven as |d,r,d,r,d,r,d,r| ...  that's because \n",
    "        # (a) there are two or more departures prior to the first return, |d,d,d,r,d,r,d,r,d,r,d,r| ... or \n",
    "        # (b) there are two or more returns post the last departure, |d,r,d,r,d,r,r,r,r| .... \n",
    "        # so the number k of repeats is \n",
    "        k = len(departures) - len(returns)\n",
    "        # and if k > 0 it's case (a)\n",
    "        # and if k < 0 it's case b\n",
    "        if k > 0: # if there are two or more departures prior to the first return \n",
    "            departures = departures[k:] # k is the number of excess departures\n",
    "        if k < 0: # if there are two or more returns post the last departure\n",
    "            returns = returns[:k] # k is the number of excess returns\n",
    "\n",
    "            \n",
    "        # Now compute the return time distribution \n",
    "        crt = returns - departures # compute the conditional return times\n",
    "        H, bins = np.histogram(crt, bins = bins, density = True) # compute the cdf\n",
    "        F = 1 - H.cumsum()*dt # each val is the probability that TR exceeds the bin in question\n",
    "    else:\n",
    "        F = np.zeros(len(bins)-1,dtype=float)\n",
    "    \n",
    "    return F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the entire unconditional return time analysis sequence \n",
    "\n",
    "def load(filename, chunksize = 5000):\n",
    "    \"\"\"filename is the original output file of the simulation\n",
    "    chunksize is the size at which to chunk the simulation output (which is huge)\n",
    "    chunksize_ed is the size at which to chunk the entrain and deposit file output \"\"\"\n",
    "    \n",
    "    entrainpath = filename[:-4]+'-entrains.dat' # generate the filenames for entrainment and depsoition \n",
    "    depositpath = filename[:-4]+'-deposits.dat' #\n",
    "    entrainfile = pd.read_csv(entrainpath, delim_whitespace = True, \n",
    "                              index_col = None, iterator = True, chunksize = chunksize)\n",
    "    depositfile = pd.read_csv(depositpath, delim_whitespace = True,\n",
    "                              index_col = None, iterator = True, chunksize = chunksize)\n",
    "    return entrainfile, depositfile # return iteratables for each file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdfcompute(filename):\n",
    "    tt0 = time.time()\n",
    "    tmax = 500*3600 # maximum return time to allow for \n",
    "    dt = 0.5 # bin size \n",
    "    bins = np.arange(0, tmax, dt) # bins \n",
    "    chunksize=1000000 # moderate chunk size \n",
    "\n",
    "    # get the statistics of m \n",
    "    cdffilename = filename[:-4]+'-rtcdf.dat'\n",
    "    mstatsfilename = filename[:-4]+'-mstats.dat'\n",
    "    m, pm = np.loadtxt(mstatsfilename).T\n",
    "    entrainfile, depositfile = load(filename,chunksize=chunksize)\n",
    "\n",
    "    # compute all of the conditional return time cdfs for each m value under consideration\n",
    "    cdf = np.zeros(len(bins)-1,dtype=float)\n",
    "    for p,mv in zip(pm,m): # iterate through all mvalues in question\n",
    "        entrainfile, depositfile = load(filename,chunksize=chunksize) # load in the files again to refresh the iterator \n",
    "        Fmv = conditional_rt(entrainfile, depositfile, mv) # calculate the rt cdf conditional to mv\n",
    "        cdf+=p*Fmv\n",
    "\n",
    "    # compute the final output which is the bin centers and the unconditional return time pdf corresponding to them \n",
    "    #cdf = (conditionals*pm.reshape(-1, 1)).sum(axis = 0) # compute the unconditional cdf\n",
    "    bins = (bins[1:]+bins[:-1])/2.0 # compute the midpoints of the bins \n",
    "\n",
    "    data = np.array((bins,cdf)).T\n",
    "    print(time.time()-tt0, ' total time up to saving at chunksize ', chunksize)\n",
    "    \n",
    "    print ('---------------------------------------')\n",
    "    t0 = time.time()\n",
    "    np.save(cdffilename, data) # numpy save method 1 \n",
    "    print(time.time()-t0, ' numpy version 1 ')\n",
    "    \n",
    "    print ('---------------------------------------')\n",
    "    t0 = time.time()\n",
    "    np.savetxt(cdffilename,data) # numpy save method 2\n",
    "    print(time.time()-t0, 'numpy version 2')\n",
    "    \n",
    "\n",
    "    return bins, cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins, cdf = cdfcompute(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "368/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so you can get a 10% speedup with a big chunksize \n",
    "bins, cdf = cdfcompute(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 277 million lines for flow condition a.. this should take about 100s for 1million\n",
    "277*100/3600 # only 8 hrs... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how many GB is 50,000,000 lines? \n",
    "21.0/277e6*50e6 # should be able to chunk at 50 million lines ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.random.random(size=(2,int(200e6))) # should be able to chunk at 100 million lines... with 8GB or 16G\n",
    "\n",
    "array.nbytes/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array[array<0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.671296296296297\n",
      "0.32060185185185186\n"
     ]
    }
   ],
   "source": [
    "# 277 million lines at least \n",
    "# 700 million lines at most \n",
    "# compute time also scales linearly with number of mvalues which ranges from 100 to 700\n",
    "\n",
    "# therefore max compute time is \n",
    "print(7*700*100/3600/24) # days assuming changes in saving overhead are small \n",
    "\n",
    "# while min compute time is about \n",
    "print(277*100/3600/24) # days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins, cdf = np.load('/home/kp/Desktop/flow_a-l_0.6-time_1000.0hr-rtcdf.dat.npy').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(bins, cdf)\n",
    "plt.ylim(1e-3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.loglog(bins, cdf)\n",
    "plt.ylim(1e-3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "# define the parameters for analysis \n",
    "tmax = 500*3600 # maximum return time to allow for \n",
    "dt = 0.5 # bin size \n",
    "bins = np.arange(0, tmax, dt) # bins \n",
    "\n",
    "# get the statistics of m \n",
    "cdffilename = filename[:-4]+'-rtcdf.dat'\n",
    "mstatsfilename = filename[:-4]+'-mstats.dat'\n",
    "m, pm = np.loadtxt(mstatsfilename).T\n",
    "\n",
    "# compute all of the conditional return time cdfs for each m value under consideration\n",
    "cdf = np.zeros(len(bins)-1,dtype=float)\n",
    "for p,mv in zip(pm,m): # iterate through all mvalues in question\n",
    "    entrainfile, depositfile = load(filename) # load in the files again to refresh the iterator \n",
    "    Fmv = conditional_rt(entrainfile, depositfile, mv) # calculate the rt cdf conditional to mv\n",
    "    cdf+=p*Fmv\n",
    "\n",
    "# compute the final output which is the bin centers and the unconditional return time pdf corresponding to them \n",
    "#cdf = (conditionals*pm.reshape(-1, 1)).sum(axis = 0) # compute the unconditional cdf\n",
    "bins = (bins[1:]+bins[:-1])/2.0 # compute the midpoints of the bins \n",
    "\n",
    "data = np.array((bins,cdf)).T\n",
    "np.savetxt(cdffilename,data)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(bins,cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "# define the parameters for analysis \n",
    "tmax = 500*3600 # maximum return time to allow for \n",
    "dt = 0.5 # bin size \n",
    "bins = np.arange(0, tmax, dt) # bins \n",
    "\n",
    "# get the statistics of m \n",
    "cdffilename = filename[:-4]+'-rtcdf.dat'\n",
    "mstatsfilename = filename[:-4]+'-mstats.dat'\n",
    "m, pm = np.loadtxt(mstatsfilename).T\n",
    "entrainfile, depositfile = load(filename)\n",
    "\n",
    "# compute all of the conditional return time cdfs for each m value under consideration\n",
    "cdf = np.zeros(len(bins)-1,dtype=float)\n",
    "for p,mv in zip(pm,m): # iterate through all mvalues in question\n",
    "    entrainfile, depositfile = load(filename) # load in the files again to refresh the iterator \n",
    "    Fmv = conditional_rt(entrainfile, depositfile, mv) # calculate the rt cdf conditional to mv\n",
    "    cdf+=p*Fmv\n",
    "\n",
    "# compute the final output which is the bin centers and the unconditional return time pdf corresponding to them \n",
    "#cdf = (conditionals*pm.reshape(-1, 1)).sum(axis = 0) # compute the unconditional cdf\n",
    "bins = (bins[1:]+bins[:-1])/2.0 # compute the midpoints of the bins \n",
    "\n",
    "data = np.array((bins,cdf)).T\n",
    "#np.savetxt(cdffilename,data)\n",
    "print(time.time()-t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
